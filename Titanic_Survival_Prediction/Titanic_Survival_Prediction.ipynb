{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMvMKn4h2J4BzoZRzkpRe7C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"u1pscy4nYT2E"},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","\"\"\"Titanic Survival Predictions\n","\n","\"\"\"\n","\n","# Commented out IPython magic to ensure Python compatibility.\n","#data analysis libraries\n","import numpy as np\n","import pandas as pd\n","\n","#visualization libraries\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","# %matplotlib inline\n","\n","#ignore warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","#import train and test CSV files\n","train = pd.read_csv(\"train.csv\")\n","test = pd.read_csv(\"test.csv\")\n","\n","#take a look at the training data\n","train.describe(include=\"all\")\n","\n","#get a list of the features within the dataset\n","print(train.columns)\n","\n","#see a sample of the dataset to get an idea of the variables\n","train.sample(5)\n","\n","#see a summary of the training dataset\n","train.describe(include = \"all\")\n","\n","#check for any other unusable values\n","print(pd.isnull(train).sum())\n","\n","#draw a bar plot of survival by sex\n","sns.barplot(x=\"Sex\", y=\"Survived\", data=train)\n","\n","#print percentages of females vs. males that survive\n","print(\"Percentage of females who survived:\", train[\"Survived\"][train[\"Sex\"] == 'female'].value_counts(normalize = True)[1]*100)\n","\n","print(\"Percentage of males who survived:\", train[\"Survived\"][train[\"Sex\"] == 'male'].value_counts(normalize = True)[1]*100)\n","\n","#draw a bar plot of survival by Pclass\n","sns.barplot(x=\"Pclass\", y=\"Survived\", data=train)\n","\n","#print percentage of people by Pclass that survived\n","print(\"Percentage of Pclass = 1 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 1].value_counts(normalize = True)[1]*100)\n","\n","print(\"Percentage of Pclass = 2 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 2].value_counts(normalize = True)[1]*100)\n","\n","print(\"Percentage of Pclass = 3 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 3].value_counts(normalize = True)[1]*100)\n","\n","#draw a bar plot for SibSp vs. survival\n","sns.barplot(x=\"SibSp\", y=\"Survived\", data=train)\n","\n","#I won't be printing individual percent values for all of these.\n","print(\"Percentage of SibSp = 0 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 0].value_counts(normalize = True)[1]*100)\n","\n","print(\"Percentage of SibSp = 1 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 1].value_counts(normalize = True)[1]*100)\n","\n","print(\"Percentage of SibSp = 2 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 2].value_counts(normalize = True)[1]*100)\n","\n","#draw a bar plot for Parch vs. survival\n","sns.barplot(x=\"Parch\", y=\"Survived\", data=train)\n","plt.show()\n","\n","#sort the ages into logical categories\n","train[\"Age\"] = train[\"Age\"].fillna(-0.5)\n","test[\"Age\"] = test[\"Age\"].fillna(-0.5)\n","bins = [-1, 0, 5, 12, 18, 24, 35, 60, np.inf]\n","labels = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\n","train['AgeGroup'] = pd.cut(train[\"Age\"], bins, labels = labels)\n","test['AgeGroup'] = pd.cut(test[\"Age\"], bins, labels = labels)\n","\n","#draw a bar plot of Age vs. survival\n","sns.barplot(x=\"AgeGroup\", y=\"Survived\", data=train)\n","plt.show()\n","\n","train[\"CabinBool\"] = (train[\"Cabin\"].notnull().astype('int'))\n","test[\"CabinBool\"] = (test[\"Cabin\"].notnull().astype('int'))\n","\n","#calculate percentages of CabinBool vs. survived\n","print(\"Percentage of CabinBool = 1 who survived:\", train[\"Survived\"][train[\"CabinBool\"] == 1].value_counts(normalize = True)[1]*100)\n","\n","print(\"Percentage of CabinBool = 0 who survived:\", train[\"Survived\"][train[\"CabinBool\"] == 0].value_counts(normalize = True)[1]*100)\n","#draw a bar plot of CabinBool vs. survival\n","sns.barplot(x=\"CabinBool\", y=\"Survived\", data=train)\n","plt.show()\n","\n","test.describe(include=\"all\")\n","\n","#we'll start off by dropping the Cabin feature since not a lot more useful information can be extracted from it.\n","train = train.drop(['Cabin'], axis = 1)\n","test = test.drop(['Cabin'], axis = 1)\n","\n","#we can also drop the Ticket feature since it's unlikely to yield any useful information\n","train = train.drop(['Ticket'], axis = 1)\n","test = test.drop(['Ticket'], axis = 1)\n","\n","#now we need to fill in the missing values in the Embarked feature\n","print(\"Number of people embarking in Southampton (S):\")\n","southampton = train[train[\"Embarked\"] == \"S\"].shape[0]\n","print(southampton)\n","\n","print(\"Number of people embarking in Cherbourg (C):\")\n","cherbourg = train[train[\"Embarked\"] == \"C\"].shape[0]\n","print(cherbourg)\n","\n","print(\"Number of people embarking in Queenstown (Q):\")\n","queenstown = train[train[\"Embarked\"] == \"Q\"].shape[0]\n","print(queenstown)\n","\n","#replacing the missing values in the Embarked feature with S\n","train = train.fillna({\"Embarked\": \"S\"})\n","\n","#create a combined group of both datasets\n","combine = [train, test]\n","\n","#extract a title for each Name in the train and test datasets\n","for dataset in combine:\n","    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n","\n","pd.crosstab(train['Title'], train['Sex'])\n","\n","#replace various titles with more common names\n","for dataset in combine:\n","    dataset['Title'] = dataset['Title'].replace(['Lady', 'Capt', 'Col',\n","    'Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\n","\n","    dataset['Title'] = dataset['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\n","    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n","    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n","    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n","\n","train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\n","\n","#map each of the title groups to a numerical value\n","title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Royal\": 5, \"Rare\": 6}\n","for dataset in combine:\n","    dataset['Title'] = dataset['Title'].map(title_mapping)\n","    dataset['Title'] = dataset['Title'].fillna(0)\n","\n","train.head()\n","\n","# fill missing age with mode age group for each title\n","mr_age = train[train[\"Title\"] == 1][\"AgeGroup\"].mode() #Young Adult\n","miss_age = train[train[\"Title\"] == 2][\"AgeGroup\"].mode() #Student\n","mrs_age = train[train[\"Title\"] == 3][\"AgeGroup\"].mode() #Adult\n","master_age = train[train[\"Title\"] == 4][\"AgeGroup\"].mode() #Baby\n","royal_age = train[train[\"Title\"] == 5][\"AgeGroup\"].mode() #Adult\n","rare_age = train[train[\"Title\"] == 6][\"AgeGroup\"].mode() #Adult\n","\n","age_title_mapping = {1: \"Young Adult\", 2: \"Student\", 3: \"Adult\", 4: \"Baby\", 5: \"Adult\", 6: \"Adult\"}\n","\n","#I tried to get this code to work with using .map(), but couldn't.\n","#I've put down a less elegant, temporary solution for now.\n","#train = train.fillna({\"Age\": train[\"Title\"].map(age_title_mapping)})\n","#test = test.fillna({\"Age\": test[\"Title\"].map(age_title_mapping)})\n","\n","for x in range(len(train[\"AgeGroup\"])):\n","    if train[\"AgeGroup\"][x] == \"Unknown\":\n","        train[\"AgeGroup\"][x] = age_title_mapping[train[\"Title\"][x]]\n","\n","for x in range(len(test[\"AgeGroup\"])):\n","    if test[\"AgeGroup\"][x] == \"Unknown\":\n","        test[\"AgeGroup\"][x] = age_title_mapping[test[\"Title\"][x]]\n","\n","#map each Age value to a numerical value\n","age_mapping = {'Baby': 1, 'Child': 2, 'Teenager': 3, 'Student': 4, 'Young Adult': 5, 'Adult': 6, 'Senior': 7}\n","train['AgeGroup'] = train['AgeGroup'].map(age_mapping)\n","test['AgeGroup'] = test['AgeGroup'].map(age_mapping)\n","\n","train.head()\n","\n","#dropping the Age feature for now, might change\n","train = train.drop(['Age'], axis = 1)\n","test = test.drop(['Age'], axis = 1)\n","\n","#drop the name feature since it contains no more useful information.\n","train = train.drop(['Name'], axis = 1)\n","test = test.drop(['Name'], axis = 1)\n","\n","#map each Sex value to a numerical value\n","sex_mapping = {\"male\": 0, \"female\": 1}\n","train['Sex'] = train['Sex'].map(sex_mapping)\n","test['Sex'] = test['Sex'].map(sex_mapping)\n","\n","train.head()\n","\n","#map each Embarked value to a numerical value\n","embarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\n","train['Embarked'] = train['Embarked'].map(embarked_mapping)\n","test['Embarked'] = test['Embarked'].map(embarked_mapping)\n","\n","train.head()\n","\n","#fill in missing Fare value in test set based on mean fare for that Pclass\n","for x in range(len(test[\"Fare\"])):\n","    if pd.isnull(test[\"Fare\"][x]):\n","        pclass = test[\"Pclass\"][x] #Pclass = 3\n","        test[\"Fare\"][x] = round(train[train[\"Pclass\"] == pclass][\"Fare\"].mean(), 4)\n","\n","#map Fare values into groups of numerical values\n","train['FareBand'] = pd.qcut(train['Fare'], 4, labels = [1, 2, 3, 4])\n","test['FareBand'] = pd.qcut(test['Fare'], 4, labels = [1, 2, 3, 4])\n","\n","#drop Fare values\n","train = train.drop(['Fare'], axis = 1)\n","test = test.drop(['Fare'], axis = 1)\n","\n","#check train data\n","train.head()\n","\n","#check test data\n","test.head()\n","\n","from sklearn.model_selection import train_test_split\n","\n","predictors = train.drop(['Survived', 'PassengerId'], axis=1)\n","target = train[\"Survived\"]\n","x_train, x_val, y_train, y_val = train_test_split(predictors, target, test_size = 0.22, random_state = 0)\n","\n","# Gaussian Naive Bayes\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import accuracy_score\n","\n","gaussian = GaussianNB()\n","gaussian.fit(x_train, y_train)\n","y_pred = gaussian.predict(x_val)\n","acc_gaussian = round(accuracy_score(y_pred, y_val) * 100, 2)\n","print(acc_gaussian)\n","\n","# Logistic Regression\n","from sklearn.linear_model import LogisticRegression\n","\n","logreg = LogisticRegression()\n","logreg.fit(x_train, y_train)\n","y_pred = logreg.predict(x_val)\n","acc_logreg = round(accuracy_score(y_pred, y_val) * 100, 2)\n","print(acc_logreg)\n","\n","# Support Vector Machines\n","from sklearn.svm import SVC\n","\n","svc = SVC()\n","svc.fit(x_train, y_train)\n","y_pred = svc.predict(x_val)\n","acc_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\n","print(acc_svc)\n","\n","# Linear SVC\n","from sklearn.svm import LinearSVC\n","\n","linear_svc = LinearSVC()\n","linear_svc.fit(x_train, y_train)\n","y_pred = linear_svc.predict(x_val)\n","acc_linear_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\n","print(acc_linear_svc)\n","\n","# Perceptron\n","from sklearn.linear_model import Perceptron\n","\n","perceptron = Perceptron()\n","perceptron.fit(x_train, y_train)\n","y_pred = perceptron.predict(x_val)\n","acc_perceptron = round(accuracy_score(y_pred, y_val) * 100, 2)\n","print(acc_perceptron)\n","\n","#Decision Tree\n","from sklearn.tree import DecisionTreeClassifier\n","\n","decisiontree = DecisionTreeClassifier()\n","decisiontree.fit(x_train, y_train)\n","y_pred = decisiontree.predict(x_val)\n","acc_decisiontree = round(accuracy_score(y_pred, y_val) * 100, 2)\n","print(acc_decisiontree)\n","\n","# Random Forest\n","from sklearn.ensemble import RandomForestClassifier\n","\n","randomforest = RandomForestClassifier()\n","randomforest.fit(x_train, y_train)\n","y_pred = randomforest.predict(x_val)\n","acc_randomforest = round(accuracy_score(y_pred, y_val) * 100, 2)\n","print(acc_randomforest)\n","\n","# KNN or k-Nearest Neighbors\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","knn = KNeighborsClassifier()\n","knn.fit(x_train, y_train)\n","y_pred = knn.predict(x_val)\n","acc_knn = round(accuracy_score(y_pred, y_val) * 100, 2)\n","print(acc_knn)\n","\n","# Stochastic Gradient Descent\n","from sklearn.linear_model import SGDClassifier\n","\n","sgd = SGDClassifier()\n","sgd.fit(x_train, y_train)\n","y_pred = sgd.predict(x_val)\n","acc_sgd = round(accuracy_score(y_pred, y_val) * 100, 2)\n","print(acc_sgd)\n","\n","# Gradient Boosting Classifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","\n","gbk = GradientBoostingClassifier()\n","gbk.fit(x_train, y_train)\n","y_pred = gbk.predict(x_val)\n","acc_gbk = round(accuracy_score(y_pred, y_val) * 100, 2)\n","print(acc_gbk)\n","\n","models = pd.DataFrame({\n","    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression',\n","              'Random Forest', 'Naive Bayes', 'Perceptron', 'Linear SVC',\n","              'Decision Tree', 'Stochastic Gradient Descent', 'Gradient Boosting Classifier'],\n","    'Score': [acc_svc, acc_knn, acc_logreg,\n","              acc_randomforest, acc_gaussian, acc_perceptron,acc_linear_svc, acc_decisiontree,\n","              acc_sgd, acc_gbk]})\n","models.sort_values(by='Score', ascending=False)\n","\n","#set ids as PassengerId and predict survival\n","ids = test['PassengerId']\n","predictions = gbk.predict(test.drop('PassengerId', axis=1))\n","\n","#set the output as a dataframe and convert to csv file named submission.csv\n","output = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\n","output.to_csv('submission.csv', index=False)"]}]}